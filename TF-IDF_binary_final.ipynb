{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "import gc\n",
    "import sklearn\n",
    "from sklearn import feature_selection, feature_extraction, naive_bayes, pipeline, metrics\n",
    "\n",
    "#SSL error fix\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#download corpuses\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data using only 'subreddit' and 'body' features\n",
    "start = time.process_time()\n",
    "cols = ['subreddit', 'body ']\n",
    "df = pd.read_csv('politics25.tsv', nrows=20000000, sep='\\t', usecols=cols)\n",
    "df = df.dropna()\n",
    "end = time.process_time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "df = df.rename(columns={'body ':'body'})\n",
    "\n",
    "#drop nontrivial subreddits\n",
    "far_left = ['chapotraphouse', 'anarchism', 'completeanarchy']\n",
    "drops = ['antiwork', 'keep_track', 'jordanpeterson','political_revolution',\n",
    "         'esist', 'politics', 'politicalhumor', 'anarcho_capitalism', 'cringeanarchy',\n",
    "         'selfawarewolves', 'the_mueller' , 'onguardforthee']\n",
    "drops = far_left + drops\n",
    "df = df[~df.subreddit.isin(drops)]\n",
    "\n",
    "#find large (>100 0000 posts) subreddits \n",
    "large_subreddits = df['subreddit'].value_counts()[df['subreddit']\n",
    "                    .value_counts()>100000].index.tolist()\n",
    "\n",
    "#drop large subreddits\n",
    "df_small = df[~df.subreddit.isin(large_subreddits)]\n",
    "\n",
    "#sample large subreddits to 100 000 posts\n",
    "for sub in large_subreddits:\n",
    "    sample = df[df.subreddit == sub].sample(100000)\n",
    "    df_small = df_small.append(sample)\n",
    "    \n",
    "#rename column\n",
    "df_small = df.rename(columns={'subreddit': 'spectrum'})\n",
    "    \n",
    "#rename subreddits based on political spectrum\n",
    "left = ['liberal', 'wayofthebern', 'ourpresident', 'libertarian',\n",
    "        'sandersforpresident', 'socialism', 'latestagecapitalism',\n",
    "        'fullcommunism', 'communism', 'neoliberal', 'goldandblack',\n",
    "        'progressive', 'latestagesocialism']\n",
    "right = ['conservative', 'republican', 'the_donald', 'thenewright']\n",
    "\n",
    "df_small['spectrum'] = df_small['spectrum'].replace(left, 'left')\n",
    "df_small['spectrum'] = df_small['spectrum'].replace(right, 'right')\n",
    "\n",
    "#sample spectrums\n",
    "df_spectrum = pd.DataFrame()\n",
    "for spec in ['left', 'right']:\n",
    "    sample = df_small[df_small.spectrum == spec].sample(100000)\n",
    "    df_spectrum = df_spectrum.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show distribution of data\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"y\", fontsize=12)\n",
    "df_spectrum[\"spectrum\"].reset_index().groupby(\"spectrum\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing function\n",
    "def preprocess(text, lemm=True, stopwords=None):\n",
    "    \n",
    "    #clean\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    #tokenize and remove stopwords\n",
    "    lst_text = text.split()\n",
    "    if stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    stopwords]\n",
    "                \n",
    "    # if lemmatise\n",
    "    if lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    # return string\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#process text\n",
    "start = time.process_time()\n",
    "df_spectrum['processed'] = df_spectrum['body'].apply(lambda x: \n",
    "                preprocess(x, lemm=True,\n",
    "                stopwords=stopwords))\n",
    "end = time.process_time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save processed data to speed up future computations\n",
    "df_spectrum.to_csv('spectrum.csv')\n",
    "\n",
    "#load preprocessed data\n",
    "#df_spectrum = pd.read_csv('spectrum.csv')\n",
    "#df_spectrum = df_spectrum.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TF-IDF\n",
    "start = time.process_time()\n",
    "\n",
    "#split data \n",
    "train, test = sklearn.model_selection.train_test_split(df_spectrum, test_size=0.3)\n",
    "\n",
    "#target\n",
    "y_train = train['spectrum'].values\n",
    "y_test = test['spectrum'].values\n",
    "\n",
    "#Tf-idf\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "#feature matrix\n",
    "vectorizer.fit(train['processed'])\n",
    "X_train = vectorizer.transform(train['processed'])\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "end = time.process_time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier\n",
    "start = time.process_time()\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "#classifier = KMeans(n_clusters=2).fit(X_train)\n",
    "\n",
    "#pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "#train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "#test\n",
    "X_test = test[\"processed\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)\n",
    "\n",
    "end = time.process_time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob[:,1])\n",
    "                            #multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
