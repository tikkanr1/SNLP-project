{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import ssl\n",
    "import re\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "from sklearn import feature_selection, feature_extraction, metrics\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessed data (tokenized, lemmatized)\n",
    "df_small = pd.read_csv('spectrum.csv')\n",
    "df_small = df_small.dropna()\n",
    "\n",
    "#split data \n",
    "train, test = sklearn.model_selection.train_test_split(df_small, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make unigrams\n",
    "def unigram(corpus):\n",
    "    lst_corpus = []\n",
    "    for string in corpus:\n",
    "        lst_words = string.split()\n",
    "        lst_corpus.append(lst_words)\n",
    "    return lst_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train data unigrams\n",
    "corpus_train = train['body']\n",
    "lst_corpus_train = unigrams(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit vord2vec\n",
    "start = time.process_time()\n",
    "nlp = gensim.models.word2vec.Word2Vec(lst_corpus_train, vector_size=300,\n",
    "        window=8, min_count=2, sg=1, epochs=30)\n",
    "end = time.process_time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model \n",
    "nlp.save(\"word2vec_keras_working.model\")\n",
    "\n",
    "#load model \n",
    "nlp = Word2Vec.load(\"word2vec_keras_working.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tokenizer\n",
    "tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                     oov_token=\"NaN\", \n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "#tokenize train text\n",
    "tokenizer.fit_on_texts(lst_corpus_train)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "\n",
    "#create sequence\n",
    "lst_text2seq_train = tokenizer.texts_to_sequences(lst_corpus_train)\n",
    "\n",
    "#padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(lst_text2seq_train, \n",
    "                    maxlen=15, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for test set\n",
    "#test unigrams\n",
    "corpus_test = test[\"body\"]\n",
    "lst_corpus_test = unigrams(corpus_test)\n",
    "\n",
    "\n",
    "#text to sequence with the fitted tokenizer\n",
    "lst_text2seq_test = tokenizer.texts_to_sequences(lst_corpus_test)\n",
    "\n",
    "#adding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(lst_text2seq_test, maxlen=15,\n",
    "             padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix = vocabulary x vector size\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
    "\n",
    "#update row with vector if word exists in the model\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    try:\n",
    "        embeddings[idx] =  nlp.wv[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code attention layer\n",
    "#source https://arxiv.org/abs/1409.0473\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "#input\n",
    "x_in = layers.Input(shape=(15,))\n",
    "\n",
    "#embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=15, trainable=False)(x_in)\n",
    "\n",
    "#apply attention\n",
    "x = attention_layer(x, neurons=15)\n",
    "\n",
    "#2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                         return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "#final dense layers\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "#compile\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode y\n",
    "y_train = train['spectrum']\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "\n",
    "#train model\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=0, \n",
    "                     validation_split=0.3)\n",
    "\n",
    "\n",
    "#plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in \n",
    "             predicted_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['spectrum']\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = sklearn.metrics\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob[:,1])\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
